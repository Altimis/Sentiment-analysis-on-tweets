{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Yassine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import tweepy as tp\n",
    "import pprint\n",
    "import pandas as pd \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from tweepy.streaming import StreamListener\n",
    "import sys\n",
    "import string\n",
    "import time\n",
    "from tweepy import Stream\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import sys,tweepy,csv,re\n",
    "from textblob import TextBlob\n",
    "\n",
    "from bokeh.io import output_file, show\n",
    "from bokeh.palettes import Category20c\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.transform import cumsum\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "from textblob_de import TextBlobDE\n",
    "\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show, curdoc\n",
    "from bokeh.models import ColumnDataSource, Select\n",
    "from bokeh.models.glyphs import Line\n",
    "from bokeh.models.widgets import Tabs, Panel\n",
    "from bokeh.layouts import row\n",
    "\n",
    "#import spacy\n",
    "#from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data:\n",
    "    \n",
    "    #Stream netflix and disney+ tweets in USA and Europ and storing it in a csv file\n",
    "    def stream(self):\n",
    "        \n",
    "        geo_usa = \"40.68908,-100.95860,1500km\"\n",
    "        geo_europe = \"51.18348,10.03954,1200km\"\n",
    "        \n",
    "        self.get_tweets(\"netflix\", 140000, geo_usa, \"europe\", \"en\", \"eng_usa\")\n",
    "        self.get_tweets(\"netflix\", 140000, geo_europe, \"europe\", \"en\", \"eng_europe\")\n",
    "        self.get_tweets(\"netflix\", 140000, geo_europe, \"europe\", \"fr\", \"fr_europe\")\n",
    "        self.get_tweets(\"netflix\", 140000, geo_europe, \"europe\", \"de\", \"ger_europe\")\n",
    "        self.get_tweets(\"netflix\", 140000, geo_europe, \"europe\", \"it\", \"it_europe\")\n",
    "        self.get_tweets(\"disneyplus\", 140000, geo_usa, \"europe\", \"en\", \"eng_usa\")\n",
    "        self.get_tweets(\"disneyplus\", 140000, geo_europe, \"europe\", \"en\", \"eng_europe\")\n",
    "        self.get_tweets(\"disneyplus\", 140000, geo_europe, \"europe\", \"fr\", \"fr_europe\")\n",
    "        self.get_tweets(\"disneyplus\", 140000, geo_europe, \"europe\", \"de\", \"ger_europe\")\n",
    "        self.get_tweets(\"disneyplus\", 140000, geo_europe, \"europe\", \"it\", \"it_europe\")\n",
    "    \n",
    "    #Stream function\n",
    "    def get_tweets(self, hashtag, count, geo, loc, lang, lang_loc):\n",
    "        \n",
    "        consumer_key = 'DmAc128zccSvw1o29Pyr4JKJL'\n",
    "        consumer_secret = '6XcEt1VpN6gJEnCDp5UQG3hjj3uhpv7WD87PTrVrZRTEbrbzvI'\n",
    "        access_token = '27680215-1ZafgTMkunrWqql9Mt8IuBKgV0RoBBVfgLEGorFSu'\n",
    "        access_token_secret = 'hw50iadVToc0v9tF2irQO5yNhGsgfCWAaLHQYz7mPFzbl'\n",
    "\n",
    "        auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "        auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "        api = tweepy.API(auth,wait_on_rate_limit=True)\t\n",
    "\n",
    "        csvFile = open('df.csv', 'a', newline='')\n",
    "\n",
    "        csvWriter = csv.writer(csvFile)\n",
    "\n",
    "        for tweet in tweepy.Cursor(api.search,q=\"disney\", count=count, lang=lang, since=\"2020-01-31\", until=\"2020-02-09\", geocode=geo).items():\n",
    "            print (tweet.created_at, tweet.text)\n",
    "            csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8'), loc, hashtag, lang_loc])\n",
    "    \n",
    "    #import data from csv file\n",
    "    def import_data(self):\n",
    "        \n",
    "        df = pd.read_csv(\"df.csv\", names =[\"date\", 'text', 'location', 'hashtag', 'lang_loc'])\n",
    "        return df\n",
    "    \n",
    "    #preprocessing the dataframe \n",
    "    def data_preproc(self, df):\n",
    "        \n",
    "        df.lang_loc.fillna('eng_usa', inplace=True)\n",
    "        df['date']=pd.to_datetime(df['date'])\n",
    "        df=df.drop_duplicates(['text'])\n",
    "        df.set_index('date', inplace=True)\n",
    "        df=df.sort_values(by=['hashtag', 'date', 'location', 'lang_loc'])\n",
    "        df = df.drop(df[df[\"lang_loc\"]==\"it_europe\"].index)\n",
    "        return df\n",
    "    \n",
    "    #preprocessing the tweets : removing stopwords and punctuation in each langage using text_process function bellow\n",
    "    def new_text(self, df):\n",
    "        \n",
    "        new_text=[]\n",
    "        \n",
    "        for (tweet, lang_loc) in zip(df.text, df.lang_loc):\n",
    "            \n",
    "            if ((lang_loc == \"eng_usa\") | (lang_loc == \"eng_europe\")):\n",
    "                new_tweet = self.text_process(tweet, \"en\")\n",
    "                new_text.append(new_tweet)\n",
    "                \n",
    "            elif (lang_loc == \"fr_europe\"):\n",
    "                new_tweet = self.text_process(tweet, \"fr\")\n",
    "                new_text.append(new_tweet)\n",
    "                \n",
    "            elif (lang_loc == \"it_europe\"):\n",
    "                new_tweet = self.text_process(tweet, \"it\")\n",
    "                new_text.append(new_tweet)\n",
    "                \n",
    "            elif (lang_loc == \"ger_europe\"):\n",
    "                new_tweet = self.text_process(tweet, \"de\")\n",
    "                new_text.append(new_tweet)\n",
    "                \n",
    "        return new_text\n",
    "    \n",
    "    def text_process(self, mess, lang):\n",
    "        \n",
    "        stopwords=get_stop_words(lang)\n",
    "        mess = [i for i in mess if i not in string.punctuation]\n",
    "        mess = \"\".join(mess)\n",
    "        mess = \"\".join(mess.split(\"b\", 1))\n",
    "        str1= \" \"\n",
    "        return (str1.join([i for i in mess.split(\" \") if (i.lower() not in stopwords)]))\n",
    "    \n",
    "    #calculating the polarity of each tweet in each langage\n",
    "    def tweet_polarity(self, df):\n",
    "        \n",
    "        polarity = []\n",
    "        for (tweet, lang) in zip(df.new_text, df[\"lang_loc\"]):\n",
    "            if ((lang == \"eng_usa\") | (lang == \"eng_europe\")):\n",
    "                analysis = TextBlob(tweet)\n",
    "                polarity.append(analysis.sentiment.polarity)\n",
    "                \n",
    "            elif (lang == \"fr_europe\"):\n",
    "                analysis = TextBlob(tweet, pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "                polarity.append(analysis.sentiment[0])\n",
    "                \n",
    "            elif (lang == \"ger_europe\"):\n",
    "                analysis = TextBlobDE(tweet)\n",
    "                polarity.append(analysis.sentiment.polarity)\n",
    "                        \n",
    "        return polarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data.import_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data.data_preproc(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"new_text\"]=data.new_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity=data.tweet_polarity(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tweet_polarity\"] = polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>lang_loc</th>\n",
       "      <th>new_text</th>\n",
       "      <th>tweet_polarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-02-01 18:40:20</td>\n",
       "      <td>b'@68shaubooker @SupesBatsy It was clear as da...</td>\n",
       "      <td>usa</td>\n",
       "      <td>disneyplus</td>\n",
       "      <td>eng_usa</td>\n",
       "      <td>68shaubooker SupesBatsy clear day motherfucka ...</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-02-01 18:49:27</td>\n",
       "      <td>b'Mdrrr ses chien y bombe grave mtn que disney...</td>\n",
       "      <td>europe</td>\n",
       "      <td>disneyplus</td>\n",
       "      <td>fr_europe</td>\n",
       "      <td>Mdrrr chien bombe grave mtn disney  arrive tar...</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-02-01 18:53:38</td>\n",
       "      <td>b'@TheHadnot I\\xe2\\x80\\x99m geeked for WandaVi...</td>\n",
       "      <td>usa</td>\n",
       "      <td>disneyplus</td>\n",
       "      <td>eng_usa</td>\n",
       "      <td>TheHadnot Ixe2x80x99m geeked WandaVision Ms Ma...</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-02-01 18:56:18</td>\n",
       "      <td>b\"RT @JVCom: Bambi serait le prochain remake l...</td>\n",
       "      <td>europe</td>\n",
       "      <td>disneyplus</td>\n",
       "      <td>fr_europe</td>\n",
       "      <td>RT JVCom Bambi prochain remake liveaction Disn...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-02-01 18:58:49</td>\n",
       "      <td>b\"Here's my new #Marvel Phase 5 video! https:/...</td>\n",
       "      <td>usa</td>\n",
       "      <td>disneyplus</td>\n",
       "      <td>eng_usa</td>\n",
       "      <td>Heres new Marvel Phase 5 video httpstcoVV617y6Q78</td>\n",
       "      <td>0.136364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text  \\\n",
       "date                                                                     \n",
       "2020-02-01 18:40:20  b'@68shaubooker @SupesBatsy It was clear as da...   \n",
       "2020-02-01 18:49:27  b'Mdrrr ses chien y bombe grave mtn que disney...   \n",
       "2020-02-01 18:53:38  b'@TheHadnot I\\xe2\\x80\\x99m geeked for WandaVi...   \n",
       "2020-02-01 18:56:18  b\"RT @JVCom: Bambi serait le prochain remake l...   \n",
       "2020-02-01 18:58:49  b\"Here's my new #Marvel Phase 5 video! https:/...   \n",
       "\n",
       "                    location     hashtag   lang_loc  \\\n",
       "date                                                  \n",
       "2020-02-01 18:40:20      usa  disneyplus    eng_usa   \n",
       "2020-02-01 18:49:27   europe  disneyplus  fr_europe   \n",
       "2020-02-01 18:53:38      usa  disneyplus    eng_usa   \n",
       "2020-02-01 18:56:18   europe  disneyplus  fr_europe   \n",
       "2020-02-01 18:58:49      usa  disneyplus    eng_usa   \n",
       "\n",
       "                                                              new_text  \\\n",
       "date                                                                     \n",
       "2020-02-01 18:40:20  68shaubooker SupesBatsy clear day motherfucka ...   \n",
       "2020-02-01 18:49:27  Mdrrr chien bombe grave mtn disney  arrive tar...   \n",
       "2020-02-01 18:53:38  TheHadnot Ixe2x80x99m geeked WandaVision Ms Ma...   \n",
       "2020-02-01 18:56:18  RT JVCom Bambi prochain remake liveaction Disn...   \n",
       "2020-02-01 18:58:49  Heres new Marvel Phase 5 video httpstcoVV617y6Q78   \n",
       "\n",
       "                     tweet_polarity  \n",
       "date                                 \n",
       "2020-02-01 18:40:20        0.100000  \n",
       "2020-02-01 18:49:27        0.400000  \n",
       "2020-02-01 18:53:38        0.600000  \n",
       "2020-02-01 18:56:18        0.000000  \n",
       "2020-02-01 18:58:49        0.136364  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 131492 entries, 2020-02-01 18:40:20 to 2020-02-08 23:59:54\n",
      "Data columns (total 6 columns):\n",
      "text              131492 non-null object\n",
      "location          131492 non-null object\n",
      "hashtag           131492 non-null object\n",
      "lang_loc          131492 non-null object\n",
      "new_text          131492 non-null object\n",
      "tweet_polarity    131492 non-null float64\n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 7.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'C:\\Users\\Yassine\\Downloads\\lab_datacollection\\df_prepared.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The visualization in the "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
