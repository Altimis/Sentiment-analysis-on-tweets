{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Yassine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import tweepy as tp\n",
    "import pprint\n",
    "import pandas as pd \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from tweepy.streaming import StreamListener\n",
    "import sys\n",
    "import string\n",
    "import time\n",
    "from tweepy import Stream\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import sys,tweepy,csv,re\n",
    "from textblob import TextBlob\n",
    "\n",
    "from bokeh.io import output_file, show\n",
    "from bokeh.palettes import Category20c\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.transform import cumsum\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "from textblob_de import TextBlobDE\n",
    "\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show, curdoc\n",
    "from bokeh.models import ColumnDataSource, Select\n",
    "from bokeh.models.glyphs import Line\n",
    "from bokeh.models.widgets import Tabs, Panel\n",
    "from bokeh.layouts import row\n",
    "\n",
    "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms,modes\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "import os\n",
    "#import spacy\n",
    "#from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data:\n",
    "    \n",
    "    #Streams netflix and disney+ tweets in USA and Europ and stors it in a csv file\n",
    "    def stream(self):\n",
    "        \n",
    "        geo_usa = \"40.68908,-100.95860,1500km\"\n",
    "        geo_europe = \"51.18348,10.03954,1200km\"\n",
    "        \n",
    "        self.get_tweets(\"netflix\", 140000, geo_usa, \"europe\", \"en\", \"eng_usa\")\n",
    "        self.get_tweets(\"netflix\", 140000, geo_europe, \"europe\", \"en\", \"eng_europe\")\n",
    "        self.get_tweets(\"netflix\", 140000, geo_europe, \"europe\", \"fr\", \"fr_europe\")\n",
    "        self.get_tweets(\"netflix\", 140000, geo_europe, \"europe\", \"de\", \"ger_europe\")\n",
    "        self.get_tweets(\"netflix\", 140000, geo_europe, \"europe\", \"it\", \"it_europe\")\n",
    "        self.get_tweets(\"disneyplus\", 140000, geo_usa, \"europe\", \"en\", \"eng_usa\")\n",
    "        self.get_tweets(\"disneyplus\", 140000, geo_europe, \"europe\", \"en\", \"eng_europe\")\n",
    "        self.get_tweets(\"disneyplus\", 140000, geo_europe, \"europe\", \"fr\", \"fr_europe\")\n",
    "        self.get_tweets(\"disneyplus\", 140000, geo_europe, \"europe\", \"de\", \"ger_europe\")\n",
    "        self.get_tweets(\"disneyplus\", 140000, geo_europe, \"europe\", \"it\", \"it_europe\")\n",
    "    \n",
    "    #Stream function\n",
    "    def get_tweets(self, hashtag, count, geo, loc, lang, lang_loc):\n",
    "        \n",
    "        consumer_key = ''\n",
    "        consumer_secret = ''\n",
    "        access_token = ''\n",
    "        access_token_secret = ''\n",
    "\n",
    "        auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "        auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "        api = tweepy.API(auth,wait_on_rate_limit=True)\t\n",
    "\n",
    "        csvFile = open('df.csv', 'a', newline='')\n",
    "\n",
    "        csvWriter = csv.writer(csvFile)\n",
    "\n",
    "        for tweet in tweepy.Cursor(api.search,q=\"disney\", count=count, lang=lang, since=\"2020-01-31\", until=\"2020-02-09\", geocode=geo).items():\n",
    "            print (tweet.created_at, tweet.text)\n",
    "            csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8'), loc, hashtag, lang_loc])\n",
    "    \n",
    "    #import data from csv file\n",
    "    def import_data(self):\n",
    "        \n",
    "        df = pd.read_csv(\"df.csv\", names =[\"date\", 'text', 'location', 'hashtag', 'lang_loc'])\n",
    "        return df\n",
    "    \n",
    "    #preprocessing the dataframe \n",
    "    def data_preproc(self, df):\n",
    "        \n",
    "        df.lang_loc.fillna('eng_usa', inplace=True)\n",
    "        df['date']=pd.to_datetime(df['date'])\n",
    "        df=df.drop_duplicates(['text'])\n",
    "        df.set_index('date', inplace=True)\n",
    "        df=df.sort_values(by=['hashtag', 'date', 'location', 'lang_loc'])\n",
    "        df = df.drop(df[df[\"lang_loc\"]==\"it_europe\"].index)\n",
    "        return df\n",
    "    \n",
    "    #preprocessing the tweets : removing stopwords and punctuation in each langage using text_process function bellow\n",
    "    def new_text(self, df):\n",
    "        \n",
    "        new_text=[]\n",
    "        \n",
    "        for (tweet, lang_loc) in zip(df.text, df.lang_loc):\n",
    "            \n",
    "            if ((lang_loc == \"eng_usa\") | (lang_loc == \"eng_europe\")):\n",
    "                new_tweet = self.text_process(tweet, \"en\")\n",
    "                new_text.append(new_tweet)\n",
    "                \n",
    "            elif (lang_loc == \"fr_europe\"):\n",
    "                new_tweet = self.text_process(tweet, \"fr\")\n",
    "                new_text.append(new_tweet)\n",
    "                \n",
    "            elif (lang_loc == \"it_europe\"):\n",
    "                new_tweet = self.text_process(tweet, \"it\")\n",
    "                new_text.append(new_tweet)\n",
    "                \n",
    "            elif (lang_loc == \"ger_europe\"):\n",
    "                new_tweet = self.text_process(tweet, \"de\")\n",
    "                new_text.append(new_tweet)\n",
    "                \n",
    "        return new_text\n",
    "    \n",
    "    def text_process(self, mess, lang):\n",
    "        \n",
    "        stopwords=get_stop_words(lang)\n",
    "        mess = [i for i in mess if i not in string.punctuation]\n",
    "        mess = \"\".join(mess)\n",
    "        mess = \"\".join(mess.split(\"b\", 1))\n",
    "        str1= \" \"\n",
    "        return (str1.join([i for i in mess.split(\" \") if (i.lower() not in stopwords)]))\n",
    "    \n",
    "    #calculate the polarity of each tweet in each langage\n",
    "    def tweet_polarity(self, df):\n",
    "        \n",
    "        polarity = []\n",
    "        for (tweet, lang) in zip(df.new_text, df[\"lang_loc\"]):\n",
    "            if ((lang == \"eng_usa\") | (lang == \"eng_europe\")):\n",
    "                analysis = TextBlob(tweet)\n",
    "                polarity.append(analysis.sentiment.polarity)\n",
    "                \n",
    "            elif (lang == \"fr_europe\"):\n",
    "                analysis = TextBlob(tweet, pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "                polarity.append(analysis.sentiment[0])\n",
    "                \n",
    "            elif (lang == \"ger_europe\"):\n",
    "                analysis = TextBlobDE(tweet)\n",
    "                polarity.append(analysis.sentiment.polarity)\n",
    "                        \n",
    "        return polarity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : I've already streamed data in another notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data.import_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data.data_preproc(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"new_text\"]=data.new_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity=data.tweet_polarity(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tweet_polarity\"] = polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet encryption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = os.urandom(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('key.key', 'wb')  \n",
    "file.write(key) \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cipher = Cipher(algorithms.AES(key), modes.ECB(), backend=default_backend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "aesEncryptor = cipher.encryptor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_encry=[]\n",
    "for tweet in df[\"new_text\"]:\n",
    "    tweet_encry = tweet.encode(\"utf-16\") + b\"E\" * (-len(tweet) % 16) \n",
    "    tweet_encry = aesEncryptor.update(tweet_encry)\n",
    "    tweets_encry.append(tweet_encry)\n",
    "    \n",
    "df[\"text_encrypted\"]=tweets_encry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['text', 'new_text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>lang_loc</th>\n",
       "      <th>tweet_polarity</th>\n",
       "      <th>text_encrypted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-02-01 18:40:20</td>\n",
       "      <td>usa</td>\n",
       "      <td>disneyplus</td>\n",
       "      <td>eng_usa</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>b'=\\x18.w\\x1e\\xfc\\xa3\\x88\\xbeD5b7\\x95\\xc8\\xdd\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-02-01 18:49:27</td>\n",
       "      <td>europe</td>\n",
       "      <td>disneyplus</td>\n",
       "      <td>fr_europe</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>b'kt\\xd2\\x0e\\x89UY\\xe5\\x1a.\\xcc\\xa3\\xae\\x1f.\\x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-02-01 18:53:38</td>\n",
       "      <td>usa</td>\n",
       "      <td>disneyplus</td>\n",
       "      <td>eng_usa</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>b'\\xe2\\xbe\\x88G`g\\x93\"\\xae\\xfd_\\x1b\\xf0,\\xbe5\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-02-01 18:56:18</td>\n",
       "      <td>europe</td>\n",
       "      <td>disneyplus</td>\n",
       "      <td>fr_europe</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b\"i\\xb1q\\xa09\\xf1k\\xbc\\x81\\x80.\\x89(i\\x87\\xbfU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-02-01 18:58:49</td>\n",
       "      <td>usa</td>\n",
       "      <td>disneyplus</td>\n",
       "      <td>eng_usa</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>b'\\xc8\\xdb\\x18\\xd4\\xa2\\x07XG\\x8d\\xe4\\n1s\\xec\\x...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    location     hashtag   lang_loc  tweet_polarity  \\\n",
       "date                                                                  \n",
       "2020-02-01 18:40:20      usa  disneyplus    eng_usa        0.100000   \n",
       "2020-02-01 18:49:27   europe  disneyplus  fr_europe        0.400000   \n",
       "2020-02-01 18:53:38      usa  disneyplus    eng_usa        0.600000   \n",
       "2020-02-01 18:56:18   europe  disneyplus  fr_europe        0.000000   \n",
       "2020-02-01 18:58:49      usa  disneyplus    eng_usa        0.136364   \n",
       "\n",
       "                                                        text_encrypted  \n",
       "date                                                                    \n",
       "2020-02-01 18:40:20  b'=\\x18.w\\x1e\\xfc\\xa3\\x88\\xbeD5b7\\x95\\xc8\\xdd\\...  \n",
       "2020-02-01 18:49:27  b'kt\\xd2\\x0e\\x89UY\\xe5\\x1a.\\xcc\\xa3\\xae\\x1f.\\x...  \n",
       "2020-02-01 18:53:38  b'\\xe2\\xbe\\x88G`g\\x93\"\\xae\\xfd_\\x1b\\xf0,\\xbe5\\...  \n",
       "2020-02-01 18:56:18  b\"i\\xb1q\\xa09\\xf1k\\xbc\\x81\\x80.\\x89(i\\x87\\xbfU...  \n",
       "2020-02-01 18:58:49  b'\\xc8\\xdb\\x18\\xd4\\xa2\\x07XG\\x8d\\xe4\\n1s\\xec\\x...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 131492 entries, 2020-02-01 18:40:20 to 2020-02-08 23:59:54\n",
      "Data columns (total 6 columns):\n",
      "text              131492 non-null object\n",
      "location          131492 non-null object\n",
      "hashtag           131492 non-null object\n",
      "lang_loc          131492 non-null object\n",
      "new_text          131492 non-null object\n",
      "tweet_polarity    131492 non-null float64\n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 7.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'C:\\Users\\Yassine\\Desktop\\Project tweets AIT JEDDI Yassine\\df_prepared.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization is in the tweet_app file "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
